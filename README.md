# Image to Music Generator

画像からその雰囲気に合った音楽のコード進行をAIが自動で生成するプロジェクトです。

## 概要

このアプリケーションは、深層学習の画像キャプション生成モデルを応用しています。入力された画像をAIが分析し、その画像が持つ「季節」や「ムード」を解釈して、オリジナルのコード進行を出力します。さらに、生成されたコード進行をその場で音声に変換し、WAVやMP3形式のファイルとして保存することができます。

## 主な機能

- **画像解析によるコード進行の生成**: 画像の内容を解釈し、最適なコード進行を出力します。
- **音声合成**: 生成されたコード進行を、複数の波形（正弦波、三角波、矩形波など）から選んで音楽に変換します。
- **ファイル保存**: 作成した音楽をWAV形式およびMP3形式で保存できます。

## 仕組み

本プロジェクトは、主に2つのコンポーネントで構成されています。

1.  **`ongaku.py` (AIモデル)**
    - **Encoder (エンコーダ)**: 事前学習済みの画像認識モデル`VGG16`を利用し、入力された画像から視覚的な特徴量を抽出します。
    - **Decoder (デコーダ)**: `GRU`と`Attention`機構を組み合わせた系列変換モデルです。エンコーダが抽出した画像特徴量をもとに、対応するコードを1つずつ順番に生成します。

2.  **`oto.ipynb` (実行ノートブック)**
    - `ongaku.py`で定義されたAIモデルを読み込みます。
    - ユーザーに画像ファイルのパス入力を促し、モデルを使ってコード進行を生成します。
    - 生成されたコード進行を、指定された音色（波形）で音声データに変換・再生し、ファイルに保存する役割を担います。

## 動作環境

- Python
- 必要なライブラリは `requirements.txt` に記載されています。

以下のコマンドで、必要なライブラリをインストールしてください。
```bash
pip install -r requirements.txt
```

## 使い方

1.  `oto.ipynb` をJupyter NotebookやGoogle Colabで開き、上から順番にセルを実行します。
2.  途中で画像のパスや音色を尋ねられるので、指示に従って入力してください。
3.  実行が完了すると、音楽が再生され、指定したファイル名で音声ファイルが保存されます。

## ファイル構成

```
.
├── checkpoints/
│   └── train/
│       └── (学習済みモデルファイル)
├── ongaku.py          # AIモデルの定義スクリプト
├── oto.ipynb          # 音楽生成・保存を実行するノートブック
├── requirements.txt   # 必要なライブラリの一覧
└── README.md          # このファイル
```