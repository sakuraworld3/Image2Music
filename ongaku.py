# -*- coding: utf-8 -*-
"""ongaku.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eqYKneiaNN5erg2If-Ox0g0dvzGmF_wv
"""

from google.colab import drive
drive.mount('/content/drive/')
image_folder = "/content/drive/MyDrive/Datasets/Images"
music_folder = "/content/drive/MyDrive/Datasets/ChordProgressions"

"""ライブラリを読み込む"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import re
import os
import time
import json
import pickle
import pathlib
from glob import glob
from PIL import Image
import matplotlib.pyplot as plt
# %matplotlib inline

# #　精度評価関連
# from sklearn.model_selection import StratifiedKFold
# from sklearn.metrics import accuracy_score
# from sklearn.metrics import f1_score

# 学習・テストに分割するモジュールと、シャッフルするモジュール
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle

# 深層学習のライブラリ
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input

# 枚数確認
image_root = pathlib.Path(image_folder)
music_root = pathlib.Path(music_folder)
# image_root内のサブディレクトリの名前をソートしてリストに格納
label_names = sorted(item.name for item in image_root.glob('*/') if item.is_dir())

# フォルダ内のすべてのファイルのパスをリストに格納
all_image_path = list(image_root.glob("*/*.jpg")) #画像
all_music_path = list(music_root.glob("*/*")) # コード進行

# ファイルパスがキー、ディレクトリ名（春夏秋冬）が値の辞書型リスト
all_image_dict = {str(v): pathlib.Path(str(v)).parent.name for v in all_image_path}
# コード進行のテキストがキー、ディレクトリ名が値の辞書型
all_music_dict = {"<start>" + ("\"" + open(str(v), "r", encoding="utf-8").read() + "\"").replace(" ","") + "<end>": pathlib.Path(str(v)).parent.name for v in all_music_path}

"""画像ファイルのリストと、前記のリストに季節が一致するコード進行のリストを作成<br>
全ての画像に対してコード進行（テキスト）を割り当てる
"""

# コード進行のリストを作成
def make_music_list(key, all_music_dict=all_music_dict):
    return [k for k, v in all_music_dict.items() if v == key]

def make_data_list(all_image_path):
    # all_image_dict = {str(v): pathlib.Path(str(v)).parent.name for v in all_image_path}
    all_image_data = []
    all_music_data = []
    for img_k, img_v in all_image_dict.items():
      tmp = make_music_list(img_v)
      if tmp == []:
        continue
      else:
        for code in tmp:
          all_image_data.append(img_k) #画像ファイルパス
          all_music_data.append(code) #コード進行
    return all_image_data, all_music_data

all_image_data, all_code_data = make_data_list(all_image_path)
# all_image_data, all_code_data

"""画像の前処理を行う関数"""

def load_image(image_path):
  img = tf.io.read_file(image_path)
  img = tf.image.decode_jpeg(img, channels=3) # 画像データをテンソルに変換, channels=3はRGB形式
  img = tf.image.resize(img, (224, 224))
  img = preprocess_input(img)
  return img, image_path

"""特徴量を出力するモデルを作成"""

# VGG16モデルの読み込み
image_model = VGG16(include_top=False, weights='imagenet')
hidden_layer = image_model.layers[-1].output # VGG16モデルの全結合層の最後の層から特徴量を取得
# hidden_layer = image_model.get_layer('block5_conv3').output
# 新しいモデルを作る
image_features_extract_model = tf.keras.Model(inputs=image_model.input, outputs=hidden_layer)

"""シャッフルしたコード進行のリストと画像データのリストを作成"""

train_captions, img_name_vector = shuffle(all_code_data,
                                          all_image_data,
                                          random_state=1)
# シャッフルされたコード進行のリスト、画像データのリスト
# train_captions, img_name_vector

# 重複のない画像のリストを作成
# encode_train = sorted(set(img_name_vector))

# # TensorFlowのデータセットを作成
# image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)
# #画像の前処理
# image_dataset = image_dataset.map(
#     load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)

# for img, path in image_dataset:
#     batch_features = image_features_extract_model(img) #モデルを使用して特徴量を抽出
#     # 一次元のベクトルに変換
#     batch_features = tf.reshape(batch_features,
#                               (batch_features.shape[0], -1, batch_features.shape[3]))
#     # 画像ファイルのパスに対応する特徴量を.npy形式で保存
#     for bf, p in zip(batch_features, path):
#         path_of_feature = p.numpy().decode("utf-8")
#         np.save(path_of_feature, bf.numpy())
# # bf

"""コード進行の前処理（トークン化）<br>単語からインデックス、インデックスから単語への対応表作成"""

# ターゲットテンソルの最大長を計算
def max_length(tensor):
    return max(len(t) for t in tensor)

def tokenize(lang):
  # train_captionsからボキャブラリ中のトップ 5000 語を選択
  top_k = 5000
  # テキストデータをトークン化
  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(
													num_words=top_k,
													oov_token="<unk>",
													filters='!"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')

  lang_tokenizer.fit_on_texts(lang)

  tensor = lang_tokenizer.texts_to_sequences(lang) # テキストを変換
  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,
                                                         padding='post') # サイズを合わせたテンソルにする
  return tensor, lang_tokenizer

cap_vector, tokenizer = tokenize(train_captions)
# max_length = max_length(cap_vector)
# max_length

# # 学習データとテストデータを8:2に分割
# img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,
#                                                                     cap_vector,
#                                                                     test_size=0.2,
#                                                                     random_state=0)
# # データ数を表示
# print("学習データ : {}".format(len(img_name_train)))
# print("テストデータ : {}".format(len(img_name_val)))

# BATCH_SIZE = 64
# BUFFER_SIZE = len(img_name_train) # シャッフルするときに使用されるバッファサイズ
# embedding_dim = 256 # 単語埋め込みの次元数
# units = 512
# vocab_size = len(tokenizer.word_index) + 1 #テキストデータのサイズ
# steps_per_epoch = len(img_name_train) // BATCH_SIZE #イテレーションのステップ数

# features_shape = 1000 #VGG16の特徴量の次元数（）
# attention_features_shape = 49 # attentionの特徴量の次元数
# vocab_size

# # numpy ファイルをロード
# def map_func(img_name, cap):
#   img_tensor = np.load(img_name.decode('utf-8')+'.npy')
#   return img_tensor, cap

# # 画像ファイル（入力）とコード進行（出力）のペアのデータセット
# dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))

# # numpy ファイルを並列に読み込むために map を使用
# # 画像データをロードして特徴量とコード進行を取得
# dataset = dataset.map(lambda item1, item2: tf.numpy_function(
#           map_func, [item1, item2], [tf.float32, tf.int32]),
#           num_parallel_calls=tf.data.experimental.AUTOTUNE)

# # シャッフルとバッチ化
# dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
# dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

# example_input_batch, example_target_batch = next(iter(dataset))
# example_input_batch.shape, example_target_batch.shape

"""Encoder-Decoderモデルを実装"""

#attention（重みづけ）
class BahdanauAttention(tf.keras.Model):
  # パラメータの初期化
  def __init__(self, units):
    super(BahdanauAttention, self).__init__()
    self.W1 = tf.keras.layers.Dense(units) #入力特徴量の密結合層
    self.W2 = tf.keras.layers.Dense(units) #出力特徴量の密結合層
    self.V = tf.keras.layers.Dense(1) #アテンションスコアを計算する密結合層
#
  def call(self, features, hidden):
    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)

    # hidden shape == (batch_size, hidden_size)
    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)
    hidden_with_time_axis = tf.expand_dims(hidden, 1) #hiddenの次元を追加

    # score shape == (batch_size, 64, hidden_size)
    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))

    # attention_weights shape == (batch_size, 64, 1)
    # score を self.V に適用するので、最後の軸は 1 となる
    attention_weights = tf.nn.softmax(self.V(score), axis=1) #正規化したアテンションの重み

    # 合計をとったあとの　context_vector の shpae == (batch_size, hidden_size)
    # 特徴量にアテンションの重みをかけた合計
    context_vector = attention_weights * features
    context_vector = tf.reduce_sum(context_vector, axis=1)

    return context_vector, attention_weights

#  pickle 形式で保存した特徴量を渡して処理する
class Encoder(tf.keras.Model):
    # embedding_dimは出力の次元数
    def __init__(self, embedding_dim):
    # def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):
        super(Encoder, self).__init__()
        # self.batch_sz = BATCH_SIZE
        # self.enc_units = units
        # shape after fc == (batch_size, 64, embedding_dim)
        self.fc = tf.keras.layers.Dense(embedding_dim)

    def call(self, x):
      x = self.fc(x)
      x = tf.nn.relu(x)
      # x = (bach_size, 64, embedding_dim)
      return x

class Decoder(tf.keras.Model):
  def __init__(self, embedding_dim, units, vocab_size):
    super(Decoder, self).__init__()
    self.units = units

    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(self.units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')
    self.fc1 = tf.keras.layers.Dense(self.units)
    self.fc2 = tf.keras.layers.Dense(vocab_size)

    # アテンションのため
    self.attention = BahdanauAttention(self.units)

  def call(self, x, features, hidden):
    # アテンションを別のモデルとして定義
    context_vector, attention_weights = self.attention(features, hidden)

    # embedding 層を通過したあとの x の shape == (batch_size, 1, embedding_dim)
    x = self.embedding(x)

    # 結合後の x の shape == (batch_size, 1, embedding_dim + hidden_size)
    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)

    # 結合したベクトルを GRU に渡す
    output, state = self.gru(x)

    # shape == (batch_size, max_length, hidden_size)
    x = self.fc1(output)

    # x shape == (batch_size * max_length, hidden_size)
    x = tf.reshape(x, (-1, x.shape[2]))

    # output shape == (batch_size * max_length, vocab)
    x = self.fc2(x)

    return x, state, attention_weights

  def reset_state(self, batch_size):
    return tf.zeros((batch_size, self.units))

# """オプティマイザと損失関数の定義"""

# encoder = Encoder(embedding_dim)
# decoder = Decoder(embedding_dim, units, vocab_size)

# optimizer = tf.keras.optimizers.Adam()
# loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
#     from_logits=True, reduction='none')
# # 損失関数
# def loss_function(real, pred):
#   mask = tf.math.logical_not(tf.math.equal(real, 0))
#   loss_ = loss_object(real, pred)

#   mask = tf.cast(mask, dtype=loss_.dtype)
#   loss_ *= mask

#   return tf.reduce_mean(loss_)

# """チェックポイント"""

# checkpoint_path = "./checkpoints/train"
# checkpoint = tf.train.Checkpoint(optimizer=optimizer,
#                                  encoder=encoder,
#                                  decoder=decoder)
# checkpoint_prefix = os.path.join(checkpoint_path, "ckpt")
# ckpt_manager = tf.train.CheckpointManager(checkpoint, checkpoint_path, max_to_keep=5)

# start_epoch = 0
# if ckpt_manager.latest_checkpoint:
#   start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])

# """訓練"""

# loss_plot = []
# @tf.function
# def train_step(img_tensor, target):
#   loss = 0

#   # 画像のキャプションはその前後の画像と無関係なためバッチごとに隠れ状態を初期化
#   hidden = decoder.reset_state(batch_size=target.shape[0])

#   dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)

#   with tf.GradientTape() as tape:
#       features = encoder(img_tensor)

#       # Teacher Forcing - 正解値を次の入力として供給
#       for i in range(1, target.shape[1]):
#           # 特徴量をデコーダに渡す
#           predictions, hidden, _ = decoder(dec_input, features, hidden)

#           loss += loss_function(target[:, i], predictions)

#           # teacher forcing を使用
#           dec_input = tf.expand_dims(target[:, i], 1)

#   total_loss = (loss / int(target.shape[1]))

#   trainable_variables = encoder.trainable_variables + decoder.trainable_variables

#   gradients = tape.gradient(loss, trainable_variables)

#   optimizer.apply_gradients(zip(gradients, trainable_variables))

#   return loss, total_loss

# EPOCHS = 30

# for epoch in range(start_epoch, EPOCHS):
#     start = time.time()
#     total_loss = 0

#     for (batch, (img_tensor, target)) in enumerate(dataset):
#         batch_loss, t_loss = train_step(img_tensor, target)
#         total_loss += t_loss

#         if batch % 100 == 0:
#             print ('Epoch {} Batch {} Loss {:.4f}'.format(
#               epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))
#     # 後ほどグラフ化するためにエポックごとに損失を保存
#     loss_plot.append(total_loss / steps_per_epoch)

#     if epoch % 5 == 0:
#       ckpt_manager.save()

#     print ('Epoch {} Loss {:.6f}'.format(epoch + 1,
#                                          total_loss/steps_per_epoch))
#     print ('Time taken for 1 epoch {} sec\n'.format(time.time() - start))

# plt.plot(loss_plot)
# plt.xlabel('Epochs')
# plt.ylabel('Loss')
# plt.title('Loss Plot')
# plt.show()

"""翻訳"""

def evaluate(image):
# attentionの重みを初期化
    attention_plot = np.zeros((max_length, attention_features_shape))
#decoderをリセット
    hidden = decoder.reset_state(batch_size=1)
#与えられた画像の特徴量を抽出
    temp_input = tf.expand_dims(load_image(image)[0], 0)
    img_tensor_val = image_features_extract_model(temp_input)
    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))
# 画像の特徴量をencoderに渡す
    features = encoder(img_tensor_val)

    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)
    result = []

    for i in range(max_length):
    #一つ前のトークン、特徴量、隠れ状態をdecoderに渡す
        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)
    # アテンションの重みを記録
        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()
    # 予測されたトークンを確率的にサンプリング
        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()
    # 予測されたトークンを結果テキストに加える
        s = tokenizer.index_word[predicted_id]
        s = s.replace('x', '#').replace('q', '♭')
        result.append(s)

        if tokenizer.index_word[predicted_id] == '<end>':
            return result, attention_plot

        dec_input = tf.expand_dims([predicted_id], 0)

    attention_plot = attention_plot[:len(result), :]
    return result, attention_plot

"""アテンションの重みをプロットする関数"""

def plot_attention(image, result, attention_plot):
    temp_image = np.array(Image.open(image))

    fig = plt.figure(figsize=(10, 10))

    len_result = len(result)
    for l in range(len_result):
        temp_att = np.resize(attention_plot[l], (8, 8))
        ax = fig.add_subplot(len_result//2, len_result//2, l+1)
        ax.set_title(result[l])
        img = ax.imshow(temp_image)
        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())

    plt.tight_layout()
    plt.show()

# # 検証
# val_id = np.random.randint(0, len(img_name_val))
# image = img_name_val[val_id]
# sentence = '|'.join([tokenizer.index_word[i] for i in cap_val[val_id] if i not in [0]])
# result, attention_plot = evaluate(image)

# print ('Input:', sentence.replace('x', '#').replace('q', '♭'))
# print ('Prediction translation:', '|'.join(result))
# plot_attention(image, result, attention_plot)

# """チェックポイントを復元してテスト"""

# # checkpoint_path の中の最後のチェックポイントを復元
# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))

def make_chord():
  image_path = input("画像のパス：")
  result, attention_plot = evaluate(image_path)
  print ('Prediction Caption:', '|'.join(result))
  plot_attention(image_path, result, attention_plot)
  # 画像を開く
  Image.open(image_path)
# make_chord()

def make_c():
  image_path = input("画像のパス：")
  result, attention_plot = evaluate(image_path)
  print ('|'.join(result))
  result = [i for i in result]
  return result

BATCH_SIZE = 64
embedding_dim = 256 # 単語埋め込みの次元数
units = 512
vocab_size = 55 #テキストデータのサイズ
features_shape = 1000 #VGG16の特徴量の次元数（）
attention_features_shape = 49 # attentionの特徴量の次元数
max_length = 14

# 画像の特徴量抽出モデル
image_model = VGG16(include_top=False, weights='imagenet')
hidden_layer = image_model.layers[-1].output
image_features_extract_model = tf.keras.Model(inputs=image_model.input, outputs=hidden_layer)

encoder = Encoder(embedding_dim)
decoder = Decoder(embedding_dim, units, vocab_size)

optimizer = tf.keras.optimizers.Adam()
checkpoint_path = "./checkpoints/train"
checkpoint = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder=encoder,
                                 decoder=decoder)
checkpoint_prefix = os.path.join(checkpoint_path, "ckpt")
ckpt_manager = tf.train.CheckpointManager(checkpoint, checkpoint_path, max_to_keep=5)

# checkpoint_path の中の最後のチェックポイントを復元
checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))

